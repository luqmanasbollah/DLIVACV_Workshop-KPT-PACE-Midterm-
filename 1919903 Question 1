QUESTION 1 [15 marks]

a) Let k, s and p be 4, 2 and 1, respectively. What will be the shape of the output of the convolutional layer? (2 marks)

import torch
import torch.nn as nn
input_img = torch.rand(1,3,10,10)
layer = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=k, stride=s, padding=p)

answer: The shape of the output is a tensor with shape (1, 12, 5, 5).

b) Batch Normalization (BN) normalizes the mean and standard deviation for each: (tick X for the correct answer) (2 marks)

 Individual feature map
 [X] Instance in the mini-batch
 Spatial dimension
 
  Which one of the following is not an advantage of Batch Normalization (BN)? (tick X for the correct answer) (2 marks)

 BN accelerates the training of deep neural networks and tackles the vanishing gradient problem.
 For every input mini-batch, we calculate different statistics. This introduces some sort of regularization.
 BN reduces the dependence of gradients on the scale of the parameters or of their initial values.
 [X] BN needs a much slower learning rate for the total architecture to converge.
 
 d) Choose the correct statement. (tick X for the correct answer) (2 marks)

 A larger kernel is preferred for information that resides globally, and a smaller kernel is preferred for information that is distributed locally.
[X] A larger kernel is preferred for information that resides locally, and a smaller kernel is preferred for information that is distributed globally.4

e) In the following network, how many learnable parameters (weights) are there? (2 marks)

model = nn.Sequential(
        nn.Linear(3,20),
        nn.ReLU(),
        nn.Linear(20,2)
        
    )
    answer: There are 100 learnable parameters.
    
Which of the following statements are true about deep neural networks? (tick X for the correct answer) (2 marks)

 [X] Deep neural networks usually require a big amount of data to be properly trained.
 [X] They can be fooled by adversarial/ noisy examples.
 [X] They are difficult to interpret and understand.
 [X] They can still be subject to biases.
 [X] They fail to understand the context of the data that they are handling.
 [X] They perform very well on individual tasks but fail to generalize to many different tasks
 
g) Run the code in the next cell.

# import numpy as np
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
%matplotlib inline

# generate synthetic data in 2D
X, Y = make_moons(500, noise=0.2)

# plot the data
plt.scatter(X[:,0], X[:,1], c=Y)
plt.title('2D Data')
plt.show()

Can this data be correctly separable using the classifier below? Why? (3 marks)

model = nn.Sequential(
nn.Linear(n_input_dim, n_output),
nn.Sigmoid()
)

answer:No, this is because The classifier provided is a simple neural network with a single linear layer and a sigmoid activation function, which is not necessarily well-suited to separating data in two dimensions.
